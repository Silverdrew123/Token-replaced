############################# large model ####################################
SST-2: accuracy
	16-13: 0.9231651376146789 	{'task_name': 'sst2', 'template': {'sentence': 'It was great terrible .', 'class_index': {'0': 3, '1': 2}, 'position': 2}, 'data_format': {'suffix': 'tsv', 'label_col': 1, 'sentence_col': [0], 'head': True}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 4, 'learning_rate': 1e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/SST-2/16-13'} 
	16-21: 0.930045871559633 	{'task_name': 'sst2', 'template': {'sentence': 'It was great terrible .', 'class_index': {'0': 3, '1': 2}, 'position': 2}, 'data_format': {'suffix': 'tsv', 'label_col': 1, 'sentence_col': [0], 'head': True}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 4, 'learning_rate': 2e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/SST-2/16-21'} 
	16-42: 0.9231651376146789 	{'task_name': 'sst2', 'template': {'sentence': 'It was great terrible .', 'class_index': {'0': 3, '1': 2}, 'position': 2}, 'data_format': {'suffix': 'tsv', 'label_col': 1, 'sentence_col': [0], 'head': True}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 8, 'learning_rate': 1e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/SST-2/16-42'} 
	16-87: 0.9231651376146789 	{'task_name': 'sst2', 'template': {'sentence': 'It was great terrible .', 'class_index': {'0': 3, '1': 2}, 'position': 2}, 'data_format': {'suffix': 'tsv', 'label_col': 1, 'sentence_col': [0], 'head': True}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 4, 'learning_rate': 4e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/SST-2/16-87'} 
	16-100: 0.9392201834862385 	{'task_name': 'sst2', 'template': {'sentence': 'It was great terrible .', 'class_index': {'0': 3, '1': 2}, 'position': 2}, 'data_format': {'suffix': 'tsv', 'label_col': 1, 'sentence_col': [0], 'head': True}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 4, 'learning_rate': 1e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/SST-2/16-100'} 
	 mean +- std: 92.8 (0.6)
sst-5: accuracy
	16-13: 0.5171945701357467 	{'task_name': 'sst5', 'template': {'sentence': 'It was terrible bad okay good great .', 'class_index': {'0': 2, '1': 3, '2': 4, '3': 5, '4': 6}, 'position': 2}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 8, 'learning_rate': 1e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/sst-5/16-13'} 
	16-21: 0.5420814479638009 	{'task_name': 'sst5', 'template': {'sentence': 'It was terrible bad okay good great .', 'class_index': {'0': 2, '1': 3, '2': 4, '3': 5, '4': 6}, 'position': 2}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 8, 'learning_rate': 2e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/sst-5/16-21'} 
	16-42: 0.4995475113122172 	{'task_name': 'sst5', 'template': {'sentence': 'It was terrible bad okay good great .', 'class_index': {'0': 2, '1': 3, '2': 4, '3': 5, '4': 6}, 'position': 2}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 8, 'learning_rate': 3e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/sst-5/16-42'} 
	16-87: 0.5221719457013575 	{'task_name': 'sst5', 'template': {'sentence': 'It was terrible bad okay good great .', 'class_index': {'0': 2, '1': 3, '2': 4, '3': 5, '4': 6}, 'position': 2}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 4, 'learning_rate': 2e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/sst-5/16-87'} 
	16-100: 0.45610859728506786 	{'task_name': 'sst5', 'template': {'sentence': 'It was terrible bad okay good great .', 'class_index': {'0': 2, '1': 3, '2': 4, '3': 5, '4': 6}, 'position': 2}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 4, 'learning_rate': 2e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/sst-5/16-100'} 
	 mean +- std: 50.7 (2.9)
mr: accuracy
	16-13: 0.8945 	{'task_name': 'mr', 'template': {'sentence': 'It was great terrible .', 'class_index': {'0': 3, '1': 2}, 'position': 2}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 8, 'learning_rate': 1e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/mr/16-13'} 
	16-21: 0.8855 	{'task_name': 'mr', 'template': {'sentence': 'It was great terrible .', 'class_index': {'0': 3, '1': 2}, 'position': 2}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 4, 'learning_rate': 1e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/mr/16-21'} 
	16-42: 0.9005 	{'task_name': 'mr', 'template': {'sentence': 'It was great terrible .', 'class_index': {'0': 3, '1': 2}, 'position': 2}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 4, 'learning_rate': 3e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/mr/16-42'} 
	16-87: 0.905 	{'task_name': 'mr', 'template': {'sentence': 'It was great terrible .', 'class_index': {'0': 3, '1': 2}, 'position': 2}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 4, 'learning_rate': 2e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/mr/16-87'} 
	16-100: 0.8835 	{'task_name': 'mr', 'template': {'sentence': 'It was great terrible .', 'class_index': {'0': 3, '1': 2}, 'position': 2}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 4, 'learning_rate': 1e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/mr/16-100'} 
	 mean +- std: 89.4 (0.8)
cr: accuracy
	16-13: 0.9055 	{'task_name': 'cr', 'template': {'sentence': 'It was great terrible .', 'class_index': {'0': 3, '1': 2}, 'position': 2}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 4, 'learning_rate': 4e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 15, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/cr/16-13'} 
	16-21: 0.8625 	{'task_name': 'cr', 'template': {'sentence': 'It was great terrible .', 'class_index': {'0': 3, '1': 2}, 'position': 2}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 4, 'learning_rate': 3e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 15, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/cr/16-21'} 
	16-42: 0.9235 	{'task_name': 'cr', 'template': {'sentence': 'It was great terrible .', 'class_index': {'0': 3, '1': 2}, 'position': 2}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 8, 'learning_rate': 3e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 15, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/cr/16-42'} 
	16-87: 0.9205 	{'task_name': 'cr', 'template': {'sentence': 'It was great terrible .', 'class_index': {'0': 3, '1': 2}, 'position': 2}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 4, 'learning_rate': 1e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 15, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/cr/16-87'} 
	16-100: 0.9145 	{'task_name': 'cr', 'template': {'sentence': 'It was great terrible .', 'class_index': {'0': 3, '1': 2}, 'position': 2}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 4, 'learning_rate': 1e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 15, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/cr/16-100'} 
	 mean +- std: 90.5 (2.2)
mpqa: accuracy
	16-13: 0.813 	{'task_name': 'mpqa', 'template': {'sentence': 'It was great terrible .', 'class_index': {'0': 3, '1': 2}, 'position': 2}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 8, 'learning_rate': 1e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 15, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/mpqa/16-13'} 
	16-21: 0.8475 	{'task_name': 'mpqa', 'template': {'sentence': 'It was great terrible .', 'class_index': {'0': 3, '1': 2}, 'position': 2}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 8, 'learning_rate': 3e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 15, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/mpqa/16-21'} 
	16-42: 0.851 	{'task_name': 'mpqa', 'template': {'sentence': 'It was great terrible .', 'class_index': {'0': 3, '1': 2}, 'position': 2}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 8, 'learning_rate': 3e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 15, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/mpqa/16-42'} 
	16-87: 0.827 	{'task_name': 'mpqa', 'template': {'sentence': 'It was great terrible .', 'class_index': {'0': 3, '1': 2}, 'position': 2}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 4, 'learning_rate': 2e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 15, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/mpqa/16-87'} 
	16-100: 0.824 	{'task_name': 'mpqa', 'template': {'sentence': 'It was great terrible .', 'class_index': {'0': 3, '1': 2}, 'position': 2}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 4, 'learning_rate': 4e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 15, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/mpqa/16-100'} 
	 mean +- std: 83.2 (1.4)
subj: accuracy
	16-13: 0.931 	{'task_name': 'subj', 'template': {'sentence': 'This is subjective objective .', 'class_index': {'0': 2, '1': 3}, 'position': 2}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 4, 'learning_rate': 4e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/subj/16-13'} 
	16-21: 0.911 	{'task_name': 'subj', 'template': {'sentence': 'This is subjective objective .', 'class_index': {'0': 2, '1': 3}, 'position': 2}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 4, 'learning_rate': 1e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/subj/16-21'} 
	16-42: 0.9165 	{'task_name': 'subj', 'template': {'sentence': 'This is subjective objective .', 'class_index': {'0': 2, '1': 3}, 'position': 2}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 8, 'learning_rate': 1e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/subj/16-42'} 
	16-87: 0.925 	{'task_name': 'subj', 'template': {'sentence': 'This is subjective objective .', 'class_index': {'0': 2, '1': 3}, 'position': 2}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 4, 'learning_rate': 2e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/subj/16-87'} 
	16-100: 0.919 	{'task_name': 'subj', 'template': {'sentence': 'This is subjective objective .', 'class_index': {'0': 2, '1': 3}, 'position': 2}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 8, 'learning_rate': 3e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/subj/16-100'} 
	 mean +- std: 92.1 (0.7)
trec: accuracy
	16-13: 0.882 	{'task_name': 'trec', 'template': {'sentence': 'Description Entity Expression Human Location Number :', 'class_index': {'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5}, 'position': 0}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 4, 'learning_rate': 4e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/trec/16-13'} 
	16-21: 0.84 	{'task_name': 'trec', 'template': {'sentence': 'Description Entity Expression Human Location Number :', 'class_index': {'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5}, 'position': 0}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 8, 'learning_rate': 5e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/trec/16-21'} 
	16-42: 0.906 	{'task_name': 'trec', 'template': {'sentence': 'Description Entity Expression Human Location Number :', 'class_index': {'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5}, 'position': 0}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 8, 'learning_rate': 1e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/trec/16-42'} 
	16-87: 0.916 	{'task_name': 'trec', 'template': {'sentence': 'Description Entity Expression Human Location Number :', 'class_index': {'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5}, 'position': 0}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 4, 'learning_rate': 5e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/trec/16-87'} 
	16-100: 0.816 	{'task_name': 'trec', 'template': {'sentence': 'Description Entity Expression Human Location Number :', 'class_index': {'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5}, 'position': 0}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 4, 'learning_rate': 4e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/trec/16-100'} 
	 mean +- std: 87.2 (3.8)
CoLA: matthews_correlation
	16-13: 0.0006407316757551285 	{'task_name': 'cola', 'template': {'sentence': 'This is correct incorrect .', 'class_index': {'0': 3, '1': 2}, 'position': 2}, 'data_format': {'suffix': 'tsv', 'label_col': 1, 'sentence_col': [3], 'head': False}, 'metric': 'matthews_correlation', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 8, 'learning_rate': 3e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/CoLA/16-13'} 
	16-21: 0.3534945168020883 	{'task_name': 'cola', 'template': {'sentence': 'This is correct incorrect .', 'class_index': {'0': 3, '1': 2}, 'position': 2}, 'data_format': {'suffix': 'tsv', 'label_col': 1, 'sentence_col': [3], 'head': False}, 'metric': 'matthews_correlation', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 4, 'learning_rate': 4e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/CoLA/16-21'} 
	16-42: 0.0704907218023942 	{'task_name': 'cola', 'template': {'sentence': 'This is correct incorrect .', 'class_index': {'0': 3, '1': 2}, 'position': 2}, 'data_format': {'suffix': 'tsv', 'label_col': 1, 'sentence_col': [3], 'head': False}, 'metric': 'matthews_correlation', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 4, 'learning_rate': 1e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/CoLA/16-42'} 
	16-87: 0.05359754889675517 	{'task_name': 'cola', 'template': {'sentence': 'This is correct incorrect .', 'class_index': {'0': 3, '1': 2}, 'position': 2}, 'data_format': {'suffix': 'tsv', 'label_col': 1, 'sentence_col': [3], 'head': False}, 'metric': 'matthews_correlation', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 4, 'learning_rate': 4e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/CoLA/16-87'} 
	16-100: 0.338009156643601 	{'task_name': 'cola', 'template': {'sentence': 'This is correct incorrect .', 'class_index': {'0': 3, '1': 2}, 'position': 2}, 'data_format': {'suffix': 'tsv', 'label_col': 1, 'sentence_col': [3], 'head': False}, 'metric': 'matthews_correlation', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 8, 'learning_rate': 3e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/CoLA/16-100'} 
	 mean +- std: 16.3 (15.1)
MNLI: accuracy
	16-13: 0.6833418237391747 	{'task_name': 'mnli', 'template': {'sentence': '? Yes Maybe No ,', 'class_index': {'entailment': 1, 'neutral': 2, 'contradiction': 3}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [8, 9], 'head': True, 'splits': {'train': 'train', 'dev': 'dev_matched', 'test': 'test_matched'}}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 4, 'learning_rate': 1e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 512, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/MNLI/16-13'} 
	16-21: 0.6950585838003056 	{'task_name': 'mnli', 'template': {'sentence': '? Yes Maybe No ,', 'class_index': {'entailment': 1, 'neutral': 2, 'contradiction': 3}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [8, 9], 'head': True, 'splits': {'train': 'train', 'dev': 'dev_matched', 'test': 'test_matched'}}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 4, 'learning_rate': 1e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 512, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/MNLI/16-21'} 
	16-42: 0.6273051451859399 	{'task_name': 'mnli', 'template': {'sentence': '? Yes Maybe No ,', 'class_index': {'entailment': 1, 'neutral': 2, 'contradiction': 3}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [8, 9], 'head': True, 'splits': {'train': 'train', 'dev': 'dev_matched', 'test': 'test_matched'}}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 4, 'learning_rate': 1e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 512, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/MNLI/16-42'} 
	16-87: 0.7063678043810494 	{'task_name': 'mnli', 'template': {'sentence': '? Yes Maybe No ,', 'class_index': {'entailment': 1, 'neutral': 2, 'contradiction': 3}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [8, 9], 'head': True, 'splits': {'train': 'train', 'dev': 'dev_matched', 'test': 'test_matched'}}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 4, 'learning_rate': 1e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 512, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/MNLI/16-87'} 
	16-100: 0.7501782985226694 	{'task_name': 'mnli', 'template': {'sentence': '? Yes Maybe No ,', 'class_index': {'entailment': 1, 'neutral': 2, 'contradiction': 3}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [8, 9], 'head': True, 'splits': {'train': 'train', 'dev': 'dev_matched', 'test': 'test_matched'}}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 4, 'learning_rate': 1e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 512, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/MNLI/16-100'} 
	 mean +- std: 69.2 (4.0)
MNLI-MM: accuracy
	16-13: 0.6996541903986981 	{'task_name': 'mnli', 'template': {'sentence': '? Yes Maybe No ,', 'class_index': {'entailment': 1, 'neutral': 2, 'contradiction': 3}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [8, 9], 'head': True, 'splits': {'train': 'train', 'dev': 'dev_matched', 'test': 'test_mismatched'}}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 4, 'learning_rate': 1e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 512, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/MNLI/16-13'} 
	16-21: 0.7101301871440195 	{'task_name': 'mnli', 'template': {'sentence': '? Yes Maybe No ,', 'class_index': {'entailment': 1, 'neutral': 2, 'contradiction': 3}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [8, 9], 'head': True, 'splits': {'train': 'train', 'dev': 'dev_matched', 'test': 'test_mismatched'}}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 4, 'learning_rate': 1e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 512, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/MNLI/16-21'} 
	16-42: 0.6556143205858421 	{'task_name': 'mnli', 'template': {'sentence': '? Yes Maybe No ,', 'class_index': {'entailment': 1, 'neutral': 2, 'contradiction': 3}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [8, 9], 'head': True, 'splits': {'train': 'train', 'dev': 'dev_matched', 'test': 'test_mismatched'}}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 4, 'learning_rate': 1e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 512, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/MNLI/16-42'} 
	16-87: 0.7191822620016274 	{'task_name': 'mnli', 'template': {'sentence': '? Yes Maybe No ,', 'class_index': {'entailment': 1, 'neutral': 2, 'contradiction': 3}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [8, 9], 'head': True, 'splits': {'train': 'train', 'dev': 'dev_matched', 'test': 'test_mismatched'}}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 4, 'learning_rate': 1e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 512, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/MNLI/16-87'} 
	16-100: 0.764646053702197 	{'task_name': 'mnli', 'template': {'sentence': '? Yes Maybe No ,', 'class_index': {'entailment': 1, 'neutral': 2, 'contradiction': 3}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [8, 9], 'head': True, 'splits': {'train': 'train', 'dev': 'dev_matched', 'test': 'test_mismatched'}}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 4, 'learning_rate': 1e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 512, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/MNLI/16-100'} 
	 mean +- std: 71.0 (3.5)
SNLI: accuracy
	16-13: 0.7437512700670595 	{'task_name': 'snli', 'template': {'sentence': '? Yes Maybe No ,', 'class_index': {'entailment': 1, 'neutral': 2, 'contradiction': 3}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [7, 8], 'head': True}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 4, 'learning_rate': 1e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/SNLI/16-13'} 
	16-21: 0.8415972363340785 	{'task_name': 'snli', 'template': {'sentence': '? Yes Maybe No ,', 'class_index': {'entailment': 1, 'neutral': 2, 'contradiction': 3}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [7, 8], 'head': True}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 4, 'learning_rate': 2e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/SNLI/16-21'} 
	16-42: 0.783478967689494 	{'task_name': 'snli', 'template': {'sentence': '? Yes Maybe No ,', 'class_index': {'entailment': 1, 'neutral': 2, 'contradiction': 3}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [7, 8], 'head': True}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 4, 'learning_rate': 1e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/SNLI/16-42'} 
	16-87: 0.807762649867913 	{'task_name': 'snli', 'template': {'sentence': '? Yes Maybe No ,', 'class_index': {'entailment': 1, 'neutral': 2, 'contradiction': 3}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [7, 8], 'head': True}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 4, 'learning_rate': 4e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/SNLI/16-87'} 
	16-100: 0.7872383661857346 	{'task_name': 'snli', 'template': {'sentence': '? Yes Maybe No ,', 'class_index': {'entailment': 1, 'neutral': 2, 'contradiction': 3}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [7, 8], 'head': True}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 4, 'learning_rate': 1e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/SNLI/16-100'} 
	 mean +- std: 79.3 (3.2)
QNLI: accuracy
	16-13: 0.7126121178839465 	{'task_name': 'qnli', 'template': {'sentence': '? Yes No ,', 'class_index': {'entailment': 1, 'not_entailment': 2}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [1, 2], 'head': True}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 4, 'learning_rate': 4e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/QNLI/16-13'} 
	16-21: 0.7051070840197694 	{'task_name': 'qnli', 'template': {'sentence': '? Yes No ,', 'class_index': {'entailment': 1, 'not_entailment': 2}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [1, 2], 'head': True}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 8, 'learning_rate': 4e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/QNLI/16-21'} 
	16-42: 0.7296357312831777 	{'task_name': 'qnli', 'template': {'sentence': '? Yes No ,', 'class_index': {'entailment': 1, 'not_entailment': 2}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [1, 2], 'head': True}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 8, 'learning_rate': 4e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/QNLI/16-42'} 
	16-87: 0.6015010067728355 	{'task_name': 'qnli', 'template': {'sentence': '? Yes No ,', 'class_index': {'entailment': 1, 'not_entailment': 2}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [1, 2], 'head': True}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 8, 'learning_rate': 1e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/QNLI/16-87'} 
	16-100: 0.7030935383488925 	{'task_name': 'qnli', 'template': {'sentence': '? Yes No ,', 'class_index': {'entailment': 1, 'not_entailment': 2}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [1, 2], 'head': True}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 4, 'learning_rate': 3e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/QNLI/16-100'} 
	 mean +- std: 69.0 (4.5)
RTE: accuracy
	16-13: 0.7653429602888087 	{'task_name': 'rte', 'template': {'sentence': '? Yes No ,', 'class_index': {'entailment': 1, 'not_entailment': 2}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [1, 2], 'head': True}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 8, 'learning_rate': 5e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 15, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/RTE/16-13'} 
	16-21: 0.6967509025270758 	{'task_name': 'rte', 'template': {'sentence': '? Yes No ,', 'class_index': {'entailment': 1, 'not_entailment': 2}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [1, 2], 'head': True}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 4, 'learning_rate': 1e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 15, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/RTE/16-21'} 
	16-42: 0.7509025270758123 	{'task_name': 'rte', 'template': {'sentence': '? Yes No ,', 'class_index': {'entailment': 1, 'not_entailment': 2}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [1, 2], 'head': True}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 8, 'learning_rate': 4e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 15, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/RTE/16-42'} 
	16-87: 0.7184115523465704 	{'task_name': 'rte', 'template': {'sentence': '? Yes No ,', 'class_index': {'entailment': 1, 'not_entailment': 2}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [1, 2], 'head': True}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 8, 'learning_rate': 2e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 15, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/RTE/16-87'} 
	16-100: 0.779783393501805 	{'task_name': 'rte', 'template': {'sentence': '? Yes No ,', 'class_index': {'entailment': 1, 'not_entailment': 2}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [1, 2], 'head': True}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 8, 'learning_rate': 4e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 15, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/RTE/16-100'} 
	 mean +- std: 74.2 (3.1)
MRPC: f1
	16-13: 0.7632508833922262 	{'task_name': 'mrpc', 'template': {'sentence': 'No Yes ,', 'class_index': {'0': 0, '1': 1}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': 0, 'sentence_col': [3, 4], 'head': True}, 'metric': 'f1', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 8, 'learning_rate': 1e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 15, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/MRPC/16-13'} 
	16-21: 0.7692307692307692 	{'task_name': 'mrpc', 'template': {'sentence': 'No Yes ,', 'class_index': {'0': 0, '1': 1}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': 0, 'sentence_col': [3, 4], 'head': True}, 'metric': 'f1', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 4, 'learning_rate': 3e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 15, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/MRPC/16-21'} 
	16-42: 0.7407407407407407 	{'task_name': 'mrpc', 'template': {'sentence': 'No Yes ,', 'class_index': {'0': 0, '1': 1}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': 0, 'sentence_col': [3, 4], 'head': True}, 'metric': 'f1', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 8, 'learning_rate': 2e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 15, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/MRPC/16-42'} 
	16-87: 0.5875 	{'task_name': 'mrpc', 'template': {'sentence': 'No Yes ,', 'class_index': {'0': 0, '1': 1}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': 0, 'sentence_col': [3, 4], 'head': True}, 'metric': 'f1', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 8, 'learning_rate': 4e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 15, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/MRPC/16-87'} 
	16-100: 0.8013029315960912 	{'task_name': 'mrpc', 'template': {'sentence': 'No Yes ,', 'class_index': {'0': 0, '1': 1}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': 0, 'sentence_col': [3, 4], 'head': True}, 'metric': 'f1', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 4, 'learning_rate': 5e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 15, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/MRPC/16-100'} 
	 mean +- std: 73.2 (7.5)
QQP: f1
	16-13: 0.7202476309430068 	{'task_name': 'qqp', 'template': {'sentence': 'No Yes ,', 'class_index': {'0': 0, '1': 1}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [3, 4], 'head': True}, 'metric': 'f1', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 8, 'learning_rate': 2e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/QQP/16-13'} 
	16-21: 0.6902398421604291 	{'task_name': 'qqp', 'template': {'sentence': 'No Yes ,', 'class_index': {'0': 0, '1': 1}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [3, 4], 'head': True}, 'metric': 'f1', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 8, 'learning_rate': 3e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/QQP/16-21'} 
	16-42: 0.6556869369369369 	{'task_name': 'qqp', 'template': {'sentence': 'No Yes ,', 'class_index': {'0': 0, '1': 1}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [3, 4], 'head': True}, 'metric': 'f1', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 4, 'learning_rate': 4e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/QQP/16-42'} 
	16-87: 0.6299738219895289 	{'task_name': 'qqp', 'template': {'sentence': 'No Yes ,', 'class_index': {'0': 0, '1': 1}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [3, 4], 'head': True}, 'metric': 'f1', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 4, 'learning_rate': 4e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/QQP/16-87'} 
	16-100: 0.7133145202668034 	{'task_name': 'qqp', 'template': {'sentence': 'No Yes ,', 'class_index': {'0': 0, '1': 1}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [3, 4], 'head': True}, 'metric': 'f1', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 4, 'learning_rate': 1e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/QQP/16-100'} 
	 mean +- std: 68.2 (3.4)
STS-B: pearsonr
	16-13: 0.7660849932727533 	{'task_name': 'stsb', 'task_type': 'regression', 'template': {'sentence': 'Yes No ,', 'class_index': {'Yl': 1, 'Yu': 0}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [7, 8], 'head': True, 'Vl_Vu': (0, 5)}, 'metric': 'pearsonr', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 8, 'learning_rate': 4e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/STS-B/16-13'} 
	16-21: 0.7610418027763955 	{'task_name': 'stsb', 'task_type': 'regression', 'template': {'sentence': 'Yes No ,', 'class_index': {'Yl': 1, 'Yu': 0}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [7, 8], 'head': True, 'Vl_Vu': (0, 5)}, 'metric': 'pearsonr', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 8, 'learning_rate': 3e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/STS-B/16-21'} 
	16-42: 0.7759191555384845 	{'task_name': 'stsb', 'task_type': 'regression', 'template': {'sentence': 'Yes No ,', 'class_index': {'Yl': 1, 'Yu': 0}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [7, 8], 'head': True, 'Vl_Vu': (0, 5)}, 'metric': 'pearsonr', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 8, 'learning_rate': 3e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/STS-B/16-42'} 
	16-87: 0.7320075096277084 	{'task_name': 'stsb', 'task_type': 'regression', 'template': {'sentence': 'Yes No ,', 'class_index': {'Yl': 1, 'Yu': 0}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [7, 8], 'head': True, 'Vl_Vu': (0, 5)}, 'metric': 'pearsonr', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 4, 'learning_rate': 4e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/STS-B/16-87'} 
	16-100: 0.6975161544267496 	{'task_name': 'stsb', 'task_type': 'regression', 'template': {'sentence': 'Yes No ,', 'class_index': {'Yl': 1, 'Yu': 0}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [7, 8], 'head': True, 'Vl_Vu': (0, 5)}, 'metric': 'pearsonr', 'seed': 1234, 'model_name': '../models/electra-large-discriminator', 'batch_size': 4, 'learning_rate': 2e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/STS-B/16-100'} 
	 mean +- std: 74.7 (2.9)



######################## base model #####################################
SST-2: accuracy
	16-13: 0.9220183486238532 	{'task_name': 'sst2', 'template': {'sentence': 'It was great terrible .', 'class_index': {'0': 3, '1': 2}, 'position': 2}, 'data_format': {'suffix': 'tsv', 'label_col': 1, 'sentence_col': [0], 'head': True}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 8, 'learning_rate': 1e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/SST-2/16-13'} 
	16-21: 0.9025229357798165 	{'task_name': 'sst2', 'template': {'sentence': 'It was great terrible .', 'class_index': {'0': 3, '1': 2}, 'position': 2}, 'data_format': {'suffix': 'tsv', 'label_col': 1, 'sentence_col': [0], 'head': True}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 4, 'learning_rate': 2e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/SST-2/16-21'} 
	16-42: 0.9208715596330275 	{'task_name': 'sst2', 'template': {'sentence': 'It was great terrible .', 'class_index': {'0': 3, '1': 2}, 'position': 2}, 'data_format': {'suffix': 'tsv', 'label_col': 1, 'sentence_col': [0], 'head': True}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 4, 'learning_rate': 1e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/SST-2/16-42'} 
	16-87: 0.9151376146788991 	{'task_name': 'sst2', 'template': {'sentence': 'It was great terrible .', 'class_index': {'0': 3, '1': 2}, 'position': 2}, 'data_format': {'suffix': 'tsv', 'label_col': 1, 'sentence_col': [0], 'head': True}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 4, 'learning_rate': 1e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/SST-2/16-87'} 
	16-100: 0.9231651376146789 	{'task_name': 'sst2', 'template': {'sentence': 'It was great terrible .', 'class_index': {'0': 3, '1': 2}, 'position': 2}, 'data_format': {'suffix': 'tsv', 'label_col': 1, 'sentence_col': [0], 'head': True}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 4, 'learning_rate': 5e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/SST-2/16-100'} 
	 mean +- std: 91.7 (0.8)
sst-5: accuracy
	16-13: 0.48914027149321265 	{'task_name': 'sst5', 'template': {'sentence': 'It was terrible bad okay good great .', 'class_index': {'0': 2, '1': 3, '2': 4, '3': 5, '4': 6}, 'position': 2}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 8, 'learning_rate': 1e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/sst-5/16-13'} 
	16-21: 0.5153846153846153 	{'task_name': 'sst5', 'template': {'sentence': 'It was terrible bad okay good great .', 'class_index': {'0': 2, '1': 3, '2': 4, '3': 5, '4': 6}, 'position': 2}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 8, 'learning_rate': 4e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/sst-5/16-21'} 
	16-42: 0.49049773755656106 	{'task_name': 'sst5', 'template': {'sentence': 'It was terrible bad okay good great .', 'class_index': {'0': 2, '1': 3, '2': 4, '3': 5, '4': 6}, 'position': 2}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 4, 'learning_rate': 1e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/sst-5/16-42'} 
	16-87: 0.49230769230769234 	{'task_name': 'sst5', 'template': {'sentence': 'It was terrible bad okay good great .', 'class_index': {'0': 2, '1': 3, '2': 4, '3': 5, '4': 6}, 'position': 2}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 4, 'learning_rate': 5e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/sst-5/16-87'} 
	16-100: 0.4986425339366516 	{'task_name': 'sst5', 'template': {'sentence': 'It was terrible bad okay good great .', 'class_index': {'0': 2, '1': 3, '2': 4, '3': 5, '4': 6}, 'position': 2}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 8, 'learning_rate': 3e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/sst-5/16-100'} 
	 mean +- std: 49.7 (1.0)
mr: accuracy
	16-13: 0.8985 	{'task_name': 'mr', 'template': {'sentence': 'It was great terrible .', 'class_index': {'0': 3, '1': 2}, 'position': 2}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 4, 'learning_rate': 2e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/mr/16-13'} 
	16-21: 0.8975 	{'task_name': 'mr', 'template': {'sentence': 'It was great terrible .', 'class_index': {'0': 3, '1': 2}, 'position': 2}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 4, 'learning_rate': 1e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/mr/16-21'} 
	16-42: 0.871 	{'task_name': 'mr', 'template': {'sentence': 'It was great terrible .', 'class_index': {'0': 3, '1': 2}, 'position': 2}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 8, 'learning_rate': 4e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/mr/16-42'} 
	16-87: 0.837 	{'task_name': 'mr', 'template': {'sentence': 'It was great terrible .', 'class_index': {'0': 3, '1': 2}, 'position': 2}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 4, 'learning_rate': 3e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/mr/16-87'} 
	16-100: 0.834 	{'task_name': 'mr', 'template': {'sentence': 'It was great terrible .', 'class_index': {'0': 3, '1': 2}, 'position': 2}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 8, 'learning_rate': 2e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/mr/16-100'} 
	 mean +- std: 86.8 (2.8)
cr: accuracy
	16-13: 0.9165 	{'task_name': 'cr', 'template': {'sentence': 'It was great terrible .', 'class_index': {'0': 3, '1': 2}, 'position': 2}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 4, 'learning_rate': 3e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 15, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/cr/16-13'} 
	16-21: 0.917 	{'task_name': 'cr', 'template': {'sentence': 'It was great terrible .', 'class_index': {'0': 3, '1': 2}, 'position': 2}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 4, 'learning_rate': 4e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 15, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/cr/16-21'} 
	16-42: 0.891 	{'task_name': 'cr', 'template': {'sentence': 'It was great terrible .', 'class_index': {'0': 3, '1': 2}, 'position': 2}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 4, 'learning_rate': 3e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 15, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/cr/16-42'} 
	16-87: 0.911 	{'task_name': 'cr', 'template': {'sentence': 'It was great terrible .', 'class_index': {'0': 3, '1': 2}, 'position': 2}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 4, 'learning_rate': 1e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 15, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/cr/16-87'} 
	16-100: 0.906 	{'task_name': 'cr', 'template': {'sentence': 'It was great terrible .', 'class_index': {'0': 3, '1': 2}, 'position': 2}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 4, 'learning_rate': 1e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 15, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/cr/16-100'} 
	 mean +- std: 90.8 (1.0)
mpqa: accuracy
	16-13: 0.873 	{'task_name': 'mpqa', 'template': {'sentence': 'It was great terrible .', 'class_index': {'0': 3, '1': 2}, 'position': 2}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 8, 'learning_rate': 5e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 15, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/mpqa/16-13'} 
	16-21: 0.8415 	{'task_name': 'mpqa', 'template': {'sentence': 'It was great terrible .', 'class_index': {'0': 3, '1': 2}, 'position': 2}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 4, 'learning_rate': 4e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 15, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/mpqa/16-21'} 
	16-42: 0.846 	{'task_name': 'mpqa', 'template': {'sentence': 'It was great terrible .', 'class_index': {'0': 3, '1': 2}, 'position': 2}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 4, 'learning_rate': 5e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 15, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/mpqa/16-42'} 
	16-87: 0.831 	{'task_name': 'mpqa', 'template': {'sentence': 'It was great terrible .', 'class_index': {'0': 3, '1': 2}, 'position': 2}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 4, 'learning_rate': 2e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 15, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/mpqa/16-87'} 
	16-100: 0.8325 	{'task_name': 'mpqa', 'template': {'sentence': 'It was great terrible .', 'class_index': {'0': 3, '1': 2}, 'position': 2}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 4, 'learning_rate': 5e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 15, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/mpqa/16-100'} 
	 mean +- std: 84.5 (1.5)
subj: accuracy
	16-13: 0.86 	{'task_name': 'subj', 'template': {'sentence': 'This is subjective objective .', 'class_index': {'0': 2, '1': 3}, 'position': 2}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 8, 'learning_rate': 4e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/subj/16-13'} 
	16-21: 0.8635 	{'task_name': 'subj', 'template': {'sentence': 'This is subjective objective .', 'class_index': {'0': 2, '1': 3}, 'position': 2}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 4, 'learning_rate': 1e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/subj/16-21'} 
	16-42: 0.8825 	{'task_name': 'subj', 'template': {'sentence': 'This is subjective objective .', 'class_index': {'0': 2, '1': 3}, 'position': 2}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 4, 'learning_rate': 1e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/subj/16-42'} 
	16-87: 0.894 	{'task_name': 'subj', 'template': {'sentence': 'This is subjective objective .', 'class_index': {'0': 2, '1': 3}, 'position': 2}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 4, 'learning_rate': 2e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/subj/16-87'} 
	16-100: 0.8775 	{'task_name': 'subj', 'template': {'sentence': 'This is subjective objective .', 'class_index': {'0': 2, '1': 3}, 'position': 2}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 4, 'learning_rate': 5e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/subj/16-100'} 
	 mean +- std: 87.5 (1.2)
trec: accuracy
	16-13: 0.83 	{'task_name': 'trec', 'template': {'sentence': 'Description Entity Expression Human Location Number :', 'class_index': {'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5}, 'position': 0}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 8, 'learning_rate': 3e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/trec/16-13'} 
	16-21: 0.83 	{'task_name': 'trec', 'template': {'sentence': 'Description Entity Expression Human Location Number :', 'class_index': {'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5}, 'position': 0}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 4, 'learning_rate': 4e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/trec/16-21'} 
	16-42: 0.836 	{'task_name': 'trec', 'template': {'sentence': 'Description Entity Expression Human Location Number :', 'class_index': {'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5}, 'position': 0}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 8, 'learning_rate': 1e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/trec/16-42'} 
	16-87: 0.856 	{'task_name': 'trec', 'template': {'sentence': 'Description Entity Expression Human Location Number :', 'class_index': {'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5}, 'position': 0}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 8, 'learning_rate': 5e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/trec/16-87'} 
	16-100: 0.76 	{'task_name': 'trec', 'template': {'sentence': 'Description Entity Expression Human Location Number :', 'class_index': {'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5}, 'position': 0}, 'data_format': {'suffix': 'csv', 'label_col': 0, 'sentence_col': [1], 'head': False}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 4, 'learning_rate': 5e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/trec/16-100'} 
	 mean +- std: 82.2 (3.3)
CoLA: matthews_correlation
	16-13: 0.1390130341567207 	{'task_name': 'cola', 'template': {'sentence': 'This is correct incorrect .', 'class_index': {'0': 3, '1': 2}, 'position': 2}, 'data_format': {'suffix': 'tsv', 'label_col': 1, 'sentence_col': [3], 'head': False}, 'metric': 'matthews_correlation', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 4, 'learning_rate': 4e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/CoLA/16-13'} 
	16-21: 0.35200571311919493 	{'task_name': 'cola', 'template': {'sentence': 'This is correct incorrect .', 'class_index': {'0': 3, '1': 2}, 'position': 2}, 'data_format': {'suffix': 'tsv', 'label_col': 1, 'sentence_col': [3], 'head': False}, 'metric': 'matthews_correlation', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 8, 'learning_rate': 5e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/CoLA/16-21'} 
	16-42: 0.21610730084622373 	{'task_name': 'cola', 'template': {'sentence': 'This is correct incorrect .', 'class_index': {'0': 3, '1': 2}, 'position': 2}, 'data_format': {'suffix': 'tsv', 'label_col': 1, 'sentence_col': [3], 'head': False}, 'metric': 'matthews_correlation', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 4, 'learning_rate': 4e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/CoLA/16-42'} 
	16-87: 0.11289137281481215 	{'task_name': 'cola', 'template': {'sentence': 'This is correct incorrect .', 'class_index': {'0': 3, '1': 2}, 'position': 2}, 'data_format': {'suffix': 'tsv', 'label_col': 1, 'sentence_col': [3], 'head': False}, 'metric': 'matthews_correlation', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 8, 'learning_rate': 3e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/CoLA/16-87'} 
	16-100: 0.41298896224364284 	{'task_name': 'cola', 'template': {'sentence': 'This is correct incorrect .', 'class_index': {'0': 3, '1': 2}, 'position': 2}, 'data_format': {'suffix': 'tsv', 'label_col': 1, 'sentence_col': [3], 'head': False}, 'metric': 'matthews_correlation', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 4, 'learning_rate': 4e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/CoLA/16-100'} 
	 mean +- std: 24.7 (11.8)
MNLI: accuracy
	16-13: 0.5987773815588385 	{'task_name': 'mnli', 'template': {'sentence': '? Yes Maybe No ,', 'class_index': {'entailment': 1, 'neutral': 2, 'contradiction': 3}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [8, 9], 'head': True, 'splits': {'train': 'train', 'dev': 'dev_matched', 'test': 'test_matched'}}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 4, 'learning_rate': 1e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 512, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/MNLI/16-13'} 
	16-21: 0.6148751910341315 	{'task_name': 'mnli', 'template': {'sentence': '? Yes Maybe No ,', 'class_index': {'entailment': 1, 'neutral': 2, 'contradiction': 3}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [8, 9], 'head': True, 'splits': {'train': 'train', 'dev': 'dev_matched', 'test': 'test_matched'}}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 4, 'learning_rate': 1e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 512, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/MNLI/16-21'} 
	16-42: 0.551808456444218 	{'task_name': 'mnli', 'template': {'sentence': '? Yes Maybe No ,', 'class_index': {'entailment': 1, 'neutral': 2, 'contradiction': 3}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [8, 9], 'head': True, 'splits': {'train': 'train', 'dev': 'dev_matched', 'test': 'test_matched'}}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 4, 'learning_rate': 1e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 512, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/MNLI/16-42'} 
	16-87: 0.5995924605196128 	{'task_name': 'mnli', 'template': {'sentence': '? Yes Maybe No ,', 'class_index': {'entailment': 1, 'neutral': 2, 'contradiction': 3}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [8, 9], 'head': True, 'splits': {'train': 'train', 'dev': 'dev_matched', 'test': 'test_matched'}}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 4, 'learning_rate': 1e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 512, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/MNLI/16-87'} 
	16-100: 0.6204788588894549 	{'task_name': 'mnli', 'template': {'sentence': '? Yes Maybe No ,', 'class_index': {'entailment': 1, 'neutral': 2, 'contradiction': 3}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [8, 9], 'head': True, 'splits': {'train': 'train', 'dev': 'dev_matched', 'test': 'test_matched'}}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 4, 'learning_rate': 1e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 512, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/MNLI/16-100'} 
	 mean +- std: 59.7 (2.4)
MNLI-MM: accuracy
	16-13: 0.6116761594792515 	{'task_name': 'mnli', 'template': {'sentence': '? Yes Maybe No ,', 'class_index': {'entailment': 1, 'neutral': 2, 'contradiction': 3}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [8, 9], 'head': True, 'splits': {'train': 'train', 'dev': 'dev_matched', 'test': 'test_mismatched'}}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 4, 'learning_rate': 5e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 512, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/MNLI/16-13'} 
	16-21: 0.6283563873067535 	{'task_name': 'mnli', 'template': {'sentence': '? Yes Maybe No ,', 'class_index': {'entailment': 1, 'neutral': 2, 'contradiction': 3}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [8, 9], 'head': True, 'splits': {'train': 'train', 'dev': 'dev_matched', 'test': 'test_mismatched'}}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 4, 'learning_rate': 4e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 512, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/MNLI/16-21'} 
	16-42: 0.5829943043124491 	{'task_name': 'mnli', 'template': {'sentence': '? Yes Maybe No ,', 'class_index': {'entailment': 1, 'neutral': 2, 'contradiction': 3}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [8, 9], 'head': True, 'splits': {'train': 'train', 'dev': 'dev_matched', 'test': 'test_mismatched'}}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 8, 'learning_rate': 1e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 512, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/MNLI/16-42'} 
	16-87: 0.6268307567127747 	{'task_name': 'mnli', 'template': {'sentence': '? Yes Maybe No ,', 'class_index': {'entailment': 1, 'neutral': 2, 'contradiction': 3}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [8, 9], 'head': True, 'splits': {'train': 'train', 'dev': 'dev_matched', 'test': 'test_mismatched'}}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 4, 'learning_rate': 3e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 512, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/MNLI/16-87'} 
	16-100: 0.6395443449959316 	{'task_name': 'mnli', 'template': {'sentence': '? Yes Maybe No ,', 'class_index': {'entailment': 1, 'neutral': 2, 'contradiction': 3}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [8, 9], 'head': True, 'splits': {'train': 'train', 'dev': 'dev_matched', 'test': 'test_mismatched'}}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 8, 'learning_rate': 5e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 512, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/MNLI/16-100'} 
	 mean +- std: 61.8 (2.0)
SNLI: accuracy
	16-13: 0.6537289168868117 	{'task_name': 'snli', 'template': {'sentence': '? Yes Maybe No ,', 'class_index': {'entailment': 1, 'neutral': 2, 'contradiction': 3}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [7, 8], 'head': True}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 4, 'learning_rate': 3e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/SNLI/16-13'} 
	16-21: 0.7419223735013208 	{'task_name': 'snli', 'template': {'sentence': '? Yes Maybe No ,', 'class_index': {'entailment': 1, 'neutral': 2, 'contradiction': 3}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [7, 8], 'head': True}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 4, 'learning_rate': 4e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/SNLI/16-21'} 
	16-42: 0.6574883153830522 	{'task_name': 'snli', 'template': {'sentence': '? Yes Maybe No ,', 'class_index': {'entailment': 1, 'neutral': 2, 'contradiction': 3}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [7, 8], 'head': True}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 4, 'learning_rate': 2e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/SNLI/16-42'} 
	16-87: 0.7026010973379394 	{'task_name': 'snli', 'template': {'sentence': '? Yes Maybe No ,', 'class_index': {'entailment': 1, 'neutral': 2, 'contradiction': 3}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [7, 8], 'head': True}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 8, 'learning_rate': 5e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/SNLI/16-87'} 
	16-100: 0.6879699248120301 	{'task_name': 'snli', 'template': {'sentence': '? Yes Maybe No ,', 'class_index': {'entailment': 1, 'neutral': 2, 'contradiction': 3}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [7, 8], 'head': True}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 8, 'learning_rate': 3e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/SNLI/16-100'} 
	 mean +- std: 68.9 (3.2)
QNLI: accuracy
	16-13: 0.6384770272743914 	{'task_name': 'qnli', 'template': {'sentence': '? Yes No ,', 'class_index': {'entailment': 1, 'not_entailment': 2}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [1, 2], 'head': True}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 8, 'learning_rate': 4e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/QNLI/16-13'} 
	16-21: 0.5965586673988651 	{'task_name': 'qnli', 'template': {'sentence': '? Yes No ,', 'class_index': {'entailment': 1, 'not_entailment': 2}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [1, 2], 'head': True}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 4, 'learning_rate': 1e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/QNLI/16-21'} 
	16-42: 0.6516565989383123 	{'task_name': 'qnli', 'template': {'sentence': '? Yes No ,', 'class_index': {'entailment': 1, 'not_entailment': 2}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [1, 2], 'head': True}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 8, 'learning_rate': 5e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/QNLI/16-42'} 
	16-87: 0.6163280248947465 	{'task_name': 'qnli', 'template': {'sentence': '? Yes No ,', 'class_index': {'entailment': 1, 'not_entailment': 2}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [1, 2], 'head': True}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 8, 'learning_rate': 2e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/QNLI/16-87'} 
	16-100: 0.589602782354018 	{'task_name': 'qnli', 'template': {'sentence': '? Yes No ,', 'class_index': {'entailment': 1, 'not_entailment': 2}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [1, 2], 'head': True}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 4, 'learning_rate': 1e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/QNLI/16-100'} 
	 mean +- std: 61.9 (2.4)
RTE: accuracy
	16-13: 0.6606498194945848 	{'task_name': 'rte', 'template': {'sentence': '? Yes No ,', 'class_index': {'entailment': 1, 'not_entailment': 2}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [1, 2], 'head': True}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 4, 'learning_rate': 4e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 15, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/RTE/16-13'} 
	16-21: 0.6028880866425993 	{'task_name': 'rte', 'template': {'sentence': '? Yes No ,', 'class_index': {'entailment': 1, 'not_entailment': 2}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [1, 2], 'head': True}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 4, 'learning_rate': 4e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 15, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/RTE/16-21'} 
	16-42: 0.5848375451263538 	{'task_name': 'rte', 'template': {'sentence': '? Yes No ,', 'class_index': {'entailment': 1, 'not_entailment': 2}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [1, 2], 'head': True}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 8, 'learning_rate': 1e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 15, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/RTE/16-42'} 
	16-87: 0.592057761732852 	{'task_name': 'rte', 'template': {'sentence': '? Yes No ,', 'class_index': {'entailment': 1, 'not_entailment': 2}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [1, 2], 'head': True}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 4, 'learning_rate': 2e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 15, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/RTE/16-87'} 
	16-100: 0.6353790613718412 	{'task_name': 'rte', 'template': {'sentence': '? Yes No ,', 'class_index': {'entailment': 1, 'not_entailment': 2}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [1, 2], 'head': True}, 'metric': 'accuracy', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 4, 'learning_rate': 2e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 15, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/RTE/16-100'} 
	 mean +- std: 61.5 (2.9)
MRPC: f1
	16-13: 0.6732283464566929 	{'task_name': 'mrpc', 'template': {'sentence': 'No Yes ,', 'class_index': {'0': 0, '1': 1}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': 0, 'sentence_col': [3, 4], 'head': True}, 'metric': 'f1', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 8, 'learning_rate': 2e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 15, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/MRPC/16-13'} 
	16-21: 0.7755834829443448 	{'task_name': 'mrpc', 'template': {'sentence': 'No Yes ,', 'class_index': {'0': 0, '1': 1}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': 0, 'sentence_col': [3, 4], 'head': True}, 'metric': 'f1', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 4, 'learning_rate': 1e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 15, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/MRPC/16-21'} 
	16-42: 0.7824497257769653 	{'task_name': 'mrpc', 'template': {'sentence': 'No Yes ,', 'class_index': {'0': 0, '1': 1}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': 0, 'sentence_col': [3, 4], 'head': True}, 'metric': 'f1', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 4, 'learning_rate': 2e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 15, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/MRPC/16-42'} 
	16-87: 0.7359454855195912 	{'task_name': 'mrpc', 'template': {'sentence': 'No Yes ,', 'class_index': {'0': 0, '1': 1}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': 0, 'sentence_col': [3, 4], 'head': True}, 'metric': 'f1', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 4, 'learning_rate': 5e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 15, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/MRPC/16-87'} 
	16-100: 0.7272727272727273 	{'task_name': 'mrpc', 'template': {'sentence': 'No Yes ,', 'class_index': {'0': 0, '1': 1}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': 0, 'sentence_col': [3, 4], 'head': True}, 'metric': 'f1', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 4, 'learning_rate': 5e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 15, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/MRPC/16-100'} 
	 mean +- std: 73.9 (3.9)
QQP: f1
	16-13: 0.6279547790339157 	{'task_name': 'qqp', 'template': {'sentence': 'No Yes ,', 'class_index': {'0': 0, '1': 1}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [3, 4], 'head': True}, 'metric': 'f1', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 8, 'learning_rate': 4e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/QQP/16-13'} 
	16-21: 0.6148680826745404 	{'task_name': 'qqp', 'template': {'sentence': 'No Yes ,', 'class_index': {'0': 0, '1': 1}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [3, 4], 'head': True}, 'metric': 'f1', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 8, 'learning_rate': 5e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/QQP/16-21'} 
	16-42: 0.5611119190576113 	{'task_name': 'qqp', 'template': {'sentence': 'No Yes ,', 'class_index': {'0': 0, '1': 1}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [3, 4], 'head': True}, 'metric': 'f1', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 8, 'learning_rate': 3e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/QQP/16-42'} 
	16-87: 0.5243845796562935 	{'task_name': 'qqp', 'template': {'sentence': 'No Yes ,', 'class_index': {'0': 0, '1': 1}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [3, 4], 'head': True}, 'metric': 'f1', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 8, 'learning_rate': 4e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/QQP/16-87'} 
	16-100: 0.5707346447209956 	{'task_name': 'qqp', 'template': {'sentence': 'No Yes ,', 'class_index': {'0': 0, '1': 1}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [3, 4], 'head': True}, 'metric': 'f1', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 8, 'learning_rate': 4e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/QQP/16-100'} 
	 mean +- std: 58.0 (3.8)
STS-B: pearsonr
	16-13: 0.653442699881256 	{'task_name': 'stsb', 'task_type': 'regression', 'template': {'sentence': 'Yes No ,', 'class_index': {'Yl': 1, 'Yu': 0}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [7, 8], 'head': True, 'Vl_Vu': (0, 5)}, 'metric': 'pearsonr', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 8, 'learning_rate': 5e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/STS-B/16-13'} 
	16-21: 0.6989523376416708 	{'task_name': 'stsb', 'task_type': 'regression', 'template': {'sentence': 'Yes No ,', 'class_index': {'Yl': 1, 'Yu': 0}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [7, 8], 'head': True, 'Vl_Vu': (0, 5)}, 'metric': 'pearsonr', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 8, 'learning_rate': 3e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/STS-B/16-21'} 
	16-42: 0.6323070359429676 	{'task_name': 'stsb', 'task_type': 'regression', 'template': {'sentence': 'Yes No ,', 'class_index': {'Yl': 1, 'Yu': 0}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [7, 8], 'head': True, 'Vl_Vu': (0, 5)}, 'metric': 'pearsonr', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 8, 'learning_rate': 1e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/STS-B/16-42'} 
	16-87: 0.7022864301416689 	{'task_name': 'stsb', 'task_type': 'regression', 'template': {'sentence': 'Yes No ,', 'class_index': {'Yl': 1, 'Yu': 0}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [7, 8], 'head': True, 'Vl_Vu': (0, 5)}, 'metric': 'pearsonr', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 8, 'learning_rate': 5e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/STS-B/16-87'} 
	16-100: 0.6430162765131827 	{'task_name': 'stsb', 'task_type': 'regression', 'template': {'sentence': 'Yes No ,', 'class_index': {'Yl': 1, 'Yu': 0}, 'position': 1}, 'data_format': {'suffix': 'tsv', 'label_col': -1, 'sentence_col': [7, 8], 'head': True, 'Vl_Vu': (0, 5)}, 'metric': 'pearsonr', 'seed': 1234, 'model_name': '../models/electra-base-discriminator', 'batch_size': 8, 'learning_rate': 3e-05, 'weight_decay': 0.002, 'epsilon': 1e-08, 'num_epoch': 30, 'max_length': 256, 'data_path': '/home/zcli/sf/prompt/datasets/data/k-shot/STS-B/16-100'} 
	 mean +- std: 66.6 (2.9)
